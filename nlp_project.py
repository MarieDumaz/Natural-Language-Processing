"""NLP_project.ipynb

Automatically generated by Google Colaboratory.

"""

!pip install transformers==3.5.1
!pip install bert_score

''' IMPORTS '''
import pandas as pd
import matplotlib.pyplot as plt
import re
import numpy as np
import random
import sklearn
import pickle 
import nltk
nltk.download('punkt')
import time
import datetime
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers import AdamW, get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler, random_split
import torch
import math
from bert_score import score

!nvidia-smi

# Set the seed value all over the place to make this reproducible.
seed_val = 1860

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# Connect to drive
from google.colab import drive
drive.mount('/content/drive')

# Read csv file and only keep the first 50,000
ALL = pd.read_csv("drive/My Drive/Colab Notebooks/ALL.csv")
ALL = ALL.dropna(subset=['AB', 'TI'])
ALL = ALL.reset_index(drop=True)
AB = ALL.AB
TI = ALL.TI

# Utility function to remove in-text citations and publishing tags
def numClean(txt):
    
    #remove references
    txt = re.sub(r'\[\d\]', '', txt)
    
    #Remove publisher and year 
    c = [m.start() for m in re.finditer(r'\(C\)', txt, re.IGNORECASE)]
    if c:
        txt = txt[0:c[-1]]
    
    #Remove publishing tag    
    pub = [m.start() for m in re.finditer(r'PUBLISHED+', txt, re.IGNORECASE)]
    if pub:
        if len(txt[pub[-1]:len(txt)]) < 70:
            txt = txt[0:pub[-1]]
    
    #Remove author-year reference    
    ref = re.findall(r'\[.+?\]', txt)
    for y in ref:
        g = re.findall(r'\[.+?\([12][09][89012][0-9]\)\]', y)
        for z in g:
            txt = txt.replace(z, '')
    
    #Remove DOI        
    txt = re.sub(r'DOI:(?:\s)?10.\d{4,9}\/(?:\d.+?|.+?)(?:\.|\]|\)|\s)', '', txt)

    return txt

# Utility function to apply numClean and remove punctuation
def cleaning(text):
        
  text = text.apply(lambda elem: numClean(elem))
    
  #remove punctuation 
  text = text.apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))

  return text

# Clean abstracts and titles
AB, TI = cleaning(AB), cleaning(TI)

# Special toeksn to add to tokenizer
special_tokens = {'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<|summarize|>']}

# Create tokenizer and add special tokens
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.add_special_tokens(special_tokens)

# Create inputs for evaluation
eval_dataset = []
for a,t in zip(AB,TI):
    encoded = tokenizer.encode('<|startoftext|>') + tokenizer.encode(a, max_length=995, truncation=True, padding='max_length') + tokenizer.encode('<|summarize|>')
    eval_dataset.append(torch.tensor(encoded))

# Create inputs for training
fine_tune_dataset = []
for a,t in zip(AB,TI):
    encoded = tokenizer.encode('<|startoftext|>') + tokenizer.encode(a, max_length=995, truncation=True, padding='max_length') + tokenizer.encode('<|summarize|>') + tokenizer.encode(t, max_length=26, truncation=True, padding='max_length') + tokenizer.encode('<|endoftext|>')
    fine_tune_dataset.append(torch.tensor(encoded))

# Utility function to split datasets
def split_dataset(ls):

  train = ls[:95946]
  val = ls[95946:101277]
  test = ls[101277:]

  return train, val, test

# Train/Valid/Test dataframes

train_tuning, val_tuning, test_tuning = split_dataset(fine_tune_dataset)
train_eval, val_eval, test_eval = split_dataset(eval_dataset)
train_TI, val_TI, test_TI = split_dataset(TI)

batch_size = 8

#Note: most of the code below was taken from https://blog.paperspace.com/generating-text-summaries-gpt-2/

# Create the DataLoaders
train_dataloader = DataLoader(
            train_tuning,
            sampler = RandomSampler(train_tuning), 
            batch_size = batch_size
        )

validation_dataloader = DataLoader(
            val_tuning, 
            sampler = SequentialSampler(val_tuning), 
            batch_size = batch_size
        )

test_dataloader = DataLoader(
            test_tuning,
            sampler = SequentialSampler(test_tuning),
            batch_size = batch_size 
        )

# When creating a new model

#model = GPT2LMHeadModel.from_pretrained("gpt2")
#model.resize_token_embeddings(len(tokenizer))

# When downloading a pre-trained model

model = GPT2LMHeadModel.from_pretrained("drive/MyDrive/Colab Notebooks/model_save/long_model1")

device = torch.device("cuda")
model.cuda()

# Training parameters
epochs = 5
learning_rate = 5e-4
warmup_steps = 1e2
epsilon = 1e-8
sample_every = 1000

# Create optimizer
optimizer = AdamW(model.parameters(),
                  lr = learning_rate,
                  eps = epsilon
                )

# Total number of training steps is [number of batches] x [number of epochs]. 
total_steps = len(train_dataloader) * epochs

# Create the learning rate scheduler.
# This changes the learning rate as the training loop progresses
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = warmup_steps, 
                                            num_training_steps = total_steps)

# Utility function to print time
def format_time(elapsed):
    return str(datetime.timedelta(seconds=int(round((elapsed)))))

# Training sequence
total_t0 = time.time()

training_stats = []

model = model.to(device)

for epoch_i in range(0, epochs):

    # ========================================
    #               Training
    # ========================================

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    t0 = time.time()

    total_train_loss = 0

    model.train()

    for step, batch in enumerate(train_dataloader):

        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)
        b_masks = batch[1].to(device)

        model.zero_grad()        

        outputs = model(  b_input_ids,
                          labels=b_labels, 
                          attention_mask = b_masks,
                          token_type_ids=None
                        )

        loss = outputs[0]  

        batch_loss = loss.item()
        total_train_loss += batch_loss

        # Get sample every x batches.
        if step % sample_every == 0 and not step == 0:

            elapsed = format_time(time.time() - t0)
            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))

            model.eval()

            sample_outputs = model.generate(
                                    bos_token_id=random.randint(1,30000),
                                    do_sample=True,   
                                    top_k=50, 
                                    max_length = 1024,
                                    top_p=0.95, 
                                )
            #for i, sample_output in enumerate(sample_outputs):
             #     print("{}: {}".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))
            
            model.train()

        loss.backward()

        optimizer.step()

        scheduler.step()

    # Calculate the average loss over all of the batches.
    avg_train_loss = total_train_loss / len(train_dataloader)       
    
    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epoch took: {:}".format(training_time))
        
    # ========================================
    #               Validation
    # ========================================

    print("")
    print("Running Validation...")

    t0 = time.time()

    model.eval()

    total_eval_loss = 0
    nb_eval_steps = 0

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        
        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)
        b_masks = batch[1].to(device)
        
        with torch.no_grad():        

            outputs  = model(b_input_ids, 
                             attention_mask = b_masks,
                            labels=b_labels)
          
            loss = outputs[0]  
            
        batch_loss = loss.item()
        total_eval_loss += batch_loss        

    avg_val_loss = total_eval_loss / len(validation_dataloader)
    
    validation_time = format_time(time.time() - t0)    

    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print("")
print("Training complete!")
print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

# Save model
model.save_pretrained("drive/MyDrive/Colab Notebooks/model_save/long_model1")

# Display floats with two decimal places.
pd.set_option('precision', 2)

# Create a DataFrame from our training statistics.
df_stats = pd.DataFrame(data=training_stats)

# Use the 'epoch' as the row index.
df_stats = df_stats.set_index('epoch')

# Display the table.
df_stats

# Save training stats
statfile = open('drive/MyDrive/Colab Notebooks/model_save/df_stats2', 'ab') 
pickle.dump(df_stats, statfile)                      
statfile.close() 
print('done')

# Plot the learning curve.
plt.plot(df_stats['Training Loss'], 'b-o', label="Training")
plt.plot(df_stats['Valid. Loss'], 'g-o', label="Validation")

# Label the plot.
plt.title("Training & Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.xticks([1, 2, 3, 4])

plt.show()

# Utility function to evaluate models
def evaluate(p):

  c=0
  t0 = time.time()

  generated = []
  titles = []

  # For each 
  for ab, ti in zip(val_eval, val_TI):
    
    sample_outputs = model.generate(
                                ab.unsqueeze(0).to(device), 
                                do_sample=True,   
                                top_k=0, 
                                max_length = 1024,
                                top_p=p, 
                                decoder_start_token_id= '<|summarize|>',
                                pad_token_id = 50256
                                )
    

    item = sample_outputs[0]
    ge = tokenizer.decode(item[997:], skip_special_tokens=True)
    if len(ge)> len(ti):
      ge = ge[:len(ti)]
    else:
      ti = ti[:len(ge)]

    titles.append(ti)
    generated.append(ge)

    if c%1000 == 0:
      print(c)
      print(format_time(time.time() - t0))

    c+=1
  
  print("Generating finished \n start bert score")

  P, R, F1 = score(generated, titles, lang="en", verbose=True)

  return generated, titles, P, R, F1

for p in [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
  generated, titles, P, R, F1 = evaluate(p)
  print(f'p = {p}\nPrecision = {P.mean()}, Recall = {R.mean()}, F1 = {F1.mean()}')

# Sequence to visualize results
prompt = train_eval[2]
ti = train_TI[2]
sample_outputs = model.generate(
                                prompt.unsqueeze(0), 
                                do_sample=True,   
                                top_k=0, 
                                max_length = 1024,
                                top_p=0.2, 
                                decoder_start_token_id= '<|summarize|>',
                                pad_token_id = 50256
                                )

item = sample_outputs[0]
ge = tokenizer.decode(item[995:], skip_special_tokens=True)
P, R, F1 = score(ge, ti, lang="en", verbose=True)

print(f"Actual title: {ti}")
print(f"Generated title: {ge}")
print(f'p = {p}\nPrecision = {P.mean()}, Recall = {R.mean()}, F1 = {F1.mean()}')

if len(ge)> len(ti):
  ge = ge[:len(ti)]
else:
  ti = ti[:len(ge)]
P, R, F1 = score(ge, ti, lang="en", verbose=True)
print(f'Precision = {P.mean()}, Recall = {R.mean()}, F1 = {F1.mean()}')